# -*- coding: utf-8 -*-
"""dataset preparation .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kJJuZ7rTqoOwBk0Q9sH26zegflSPcsQH

## Upload
"""

#uploader just in case
from google.colab import files
uploaded = files.upload()

#created with use of these: 
#https://colab.research.google.com/github/TannerGilbert/Object-Detection-and-Image-Segmentation-with-Detectron2/blob/master/Detectron2_train_and_export_model.ipynb#scrollTo=Y19zWbMmnWlz

#mount drive with dataset

"""## Set Up"""

!pip install pyyaml==5.1
import torch, torchvision
print(torch.__version__, torch.cuda.is_available())
!gcc --version

import torchvision.transforms as tf

assert torch.__version__.startswith("1.8")
print(torch.__version__)

!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.8/index.html

exit(0) #if asks to restart runtime

#if torch.cuda.is_available() = False
#runtime->change runtime type->gpu

import numpy as np
import cv2
import matplotlib.pyplot as plt

import glob
import shutil
import os
import json
from PIL import Image
import time
import random

"""## Datasets

once done, can be ignored

segmentation dataset
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/sipakmed_dataset
# %ls

# Commented out IPython magic to ensure Python compatibility.
# %mkdir detection_dataset

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/sipakmed_dataset/detection_dataset
# %mkdir /content/drive/MyDrive/sipakmed_dataset/detection_dataset/images
# %mkdir /content/drive/MyDrive/sipakmed_dataset/detection_dataset/boundaries
# %ls

glob_path = '/content/drive/MyDrive/sipakmed_dataset'
target1 = '/content/drive/MyDrive/sipakmed_dataset/detection_dataset/images'
target2 = '/content/drive/MyDrive/sipakmed_dataset/detection_dataset/boundaries'

#renaming files according to class labels

ls = os.listdir(glob_path)
for i in range(5):
  print(ls[i])
  path = os.path.join(glob_path, ls[i])
  for _, _, files in os.walk(path):
    for f in files:
      filename = os.path.join(path, f)
      newfilename = os.path.join(path, ls[i] + '_' + f)
      os.rename(filename, newfilename)

#note: if gives Errno2, range(i,5) -> range(i+1, 5)
#or create exception

#should be ~1682, 1849, 1888, 1857, 1752
print(len([name for name in os.listdir('/content/drive/MyDrive/sipakmed_dataset/im_Parabasal') if os.path.isfile(os.path.join('/content/drive/MyDrive/sipakmed_dataset/im_Parabasal', name))]))
print(len([name for name in os.listdir('/content/drive/MyDrive/sipakmed_dataset/im_Dyskeratotic') if os.path.isfile(os.path.join('/content/drive/MyDrive/sipakmed_dataset/im_Dyskeratotic', name))]))
print(len([name for name in os.listdir('/content/drive/MyDrive/sipakmed_dataset/im_Koilocytotic') if os.path.isfile(os.path.join('/content/drive/MyDrive/sipakmed_dataset/im_Koilocytotic', name))]))
print(len([name for name in os.listdir('/content/drive/MyDrive/sipakmed_dataset/im_Metaplastic') if os.path.isfile(os.path.join('/content/drive/MyDrive/sipakmed_dataset/im_Metaplastic', name))]))
print(len([name for name in os.listdir('/content/drive/MyDrive/sipakmed_dataset/im_Superficial-Intermediate') if os.path.isfile(os.path.join('/content/drive/MyDrive/sipakmed_dataset/im_Superficial-Intermediate', name))]))

#should be ~966, (8042 or 4021)
seg_len_im = len([name for name in os.listdir(target1) if os.path.isfile(os.path.join(target1, name))])
seg_len_b = len([name for name in os.listdir(target2) if os.path.isfile(os.path.join(target2, name))])
print(seg_len_im)
print(seg_len_b)

#utilitary, clean detection_dataset directories

files = glob.glob('/content/drive/MyDrive/sipakmed_dataset/detection_dataset/boundaries/*')
for f in files:
    os.remove(f)

files = glob.glob('/content/drive/MyDrive/sipakmed_dataset/detection_dataset/images/*')
for f in files:
    os.remove(f)

#debugging wrong renames
for i in range(5):
  print(ls[i])
  path = os.path.join(glob_path, ls[i])
  for _, _, files in os.walk(path):
    for f in files:
      if (f.startswith('im_Parabasal_im_Parabasal')):
          newf = f.replace('im_Parabasal_im_Parabasal', 'im_Parabasal')
          filename = os.path.join(path, f)
          newfilename = os.path.join(path, newf)
          os.rename(filename, newfilename)
      elif (f.startswith('im_Dyskeratoric_im_Dyskeratoric')):
          newf = f.replace('im_Dyskeratoric_im_Dyskeratoric', 'im_Dyskeratoric')
          filename = os.path.join(path, f)
          newfilename = os.path.join(path, newf)
          os.rename(filename, newfilename)
      elif (f.startswith('im_Koilocytotic_im_Koilocytotic')):
          newf = f.replace('im_Koilocytotic_im_Koilocytotic', 'im_Koilocytotic')
          filename = os.path.join(path, f)
          newfilename = os.path.join(path, newf)
          os.rename(filename, newfilename)
      elif (f.startswith('im_Metaplastic_im_Metaplastic')):
          newf = f.replace('im_Metaplastic_im_Metaplastic', 'im_Metaplastic')
          filename = os.path.join(path, f)
          newfilename = os.path.join(path, newf)
          os.rename(filename, newfilename)
      elif (f.startswith('im_Superficial-Intermediate_im_Superficial-Intermediate')):
          newf = f.replace('im_Superficial-Intermediate_im_Superficial-Intermediate', 'im_Superficial-Intermediate')
          filename = os.path.join(path, f)
          newfilename = os.path.join(path, newf)
          os.rename(filename, newfilename)

#copying files into segmentation_dataset
for i in range(4,5):
  print(ls[i])
  path = os.path.join(glob_path, ls[i])
  for _, _, files in os.walk(path):
    for f in files:
      if f.endswith('.bmp'):
          shutil.copyfile(os.path.join(glob_path, ls[i] + '/' + f), os.path.join(target1, f))
      else:
          shutil.copyfile(os.path.join(glob_path, ls[i] + '/' + f), os.path.join(target2, f))

#removing nuclei boundaries
for i in os.listdir(target2):
    if os.path.isfile(os.path.join(target2,i)) and 'nuc' in i:
        os.remove(os.path.join(target2,i))

"""Classification dataset"""

# Commented out IPython magic to ensure Python compatibility.
# %mkdir '/content/drive/MyDrive/sipakmed_dataset/classification_dataset'
# %mkdir '/content/drive/MyDrive/sipakmed_dataset/classification_dataset/t'
# %mkdir '/content/drive/MyDrive/sipakmed_dataset/classification_dataset/im_Koilocytotic'
# %mkdir '/content/drive/MyDrive/sipakmed_dataset/classification_dataset/im_Metaplastic'
# %mkdir '/content/drive/MyDrive/sipakmed_dataset/classification_dataset/im_Parabasal'
# %mkdir '/content/drive/MyDrive/sipakmed_dataset/classification_dataset/im_Superficial-Intermediate'

files = glob.glob('/content/drive/MyDrive/sipakmed_dataset/classification_dataset/im_Superficial-Intermediate/*')
for f in files:
    os.remove(f)

path_cl = '/content/drive/MyDrive/sipakmed_dataset/'
ls_cl = ['im_Parabasal', 'im_Koilocytotic', 'im_Superficial-Intermediate', 'im_Dyskeratotic', 'im_Metaplastic']
path_ds = '/content/drive/MyDrive/sipakmed_dataset/classification_dataset/'
for i in range(2,3):
  path_cl_tmp = path_cl + ls_cl[i] + '/CROPPED'
  path_ds_tmp = path_ds + ls_cl[i]
  print(path_cl_tmp)
  for _, _, fs in os.walk(path_cl_tmp):
    for f in fs:
      if f.endswith('.bmp'):
        shutil.copyfile(os.path.join(path_cl_tmp, f), os.path.join(path_ds_tmp, f))

#should be ~787, 813, 825, 793, 813
p_len = len([name for name in os.listdir('/content/drive/MyDrive/sipakmed_dataset/classification_dataset/im_Parabasal') 
  if os.path.isfile(os.path.join('/content/drive/MyDrive/sipakmed_dataset/classification_dataset/im_Parabasal', name))])
d_len = len([name for name in os.listdir('/content/drive/MyDrive/sipakmed_dataset/classification_dataset/im_Dyskeratotic') 
  if os.path.isfile(os.path.join('/content/drive/MyDrive/sipakmed_dataset/classification_dataset/im_Dyskeratotic', name))])
k_len = len([name for name in os.listdir('/content/drive/MyDrive/sipakmed_dataset/classification_dataset/im_Koilocytotic') 
  if os.path.isfile(os.path.join('/content/drive/MyDrive/sipakmed_dataset/classification_dataset/im_Koilocytotic', name))])
m_len = len([name for name in os.listdir('/content/drive/MyDrive/sipakmed_dataset/classification_dataset/im_Metaplastic') 
  if os.path.isfile(os.path.join('/content/drive/MyDrive/sipakmed_dataset/classification_dataset/im_Metaplastic', name))])
s_len = len([name for name in os.listdir('/content/drive/MyDrive/sipakmed_dataset/classification_dataset/im_Superficial-Intermediate') 
  if os.path.isfile(os.path.join('/content/drive/MyDrive/sipakmed_dataset/classification_dataset/im_Superficial-Intermediate', name))])

print(p_len, d_len, k_len, m_len, s_len)

def check_res(path):
  img = Image.open(path)
  tfm = tf.ToTensor()
  img = tfm(img)
  size = img.size()
  return size

#calculate mean resolution and reshape each image
def rescale(path):
  img = Image.open(path)
  
  tfm1 = tf.ToTensor()
  img = tfm1(img)
  tfm2 = tf.Compose([tf.Resize((100,100))])
  img = tfm2(img).numpy().transpose((1, 2, 0))
  img = tfm1(img)
  size = img.size()
  tfm3 = tf.ToPILImage()
  img = tfm3(img)
  img = img.save(path)

dir_list = ['/content/drive/MyDrive/sipakmed_dataset/classification_dataset/im_Dyskeratotic',
            '/content/drive/MyDrive/sipakmed_dataset/classification_dataset/im_Koilocytotic',
            '/content/drive/MyDrive/sipakmed_dataset/classification_dataset/im_Metaplastic',
            '/content/drive/MyDrive/sipakmed_dataset/classification_dataset/im_Parabasal',
            '/content/drive/MyDrive/sipakmed_dataset/classification_dataset/im_Superficial-Intermediate']

for dir in dir_list:
  print(dir)
  img_list = glob.glob(dir + '/*')
  for img in img_list:
    path = os.path.join(dir + img)
    rescale(img)

"""segementation dataset ctnd, making it coco compatible for detectron2"""

dt_dir = '/content/drive/MyDrive/sipakmed_dataset/detection_dataset'

def create_img_id(path):
    path = path.replace('/content/drive/MyDrive/sipakmed_dataset/detection_dataset/images/im_', '')
    path = path.replace('.bmp', '')
    last = path[-3:]
    first = path[0]
    id = first + last 
    return id

def path_from_img_id(img_id):
    if (img_id[0] == 'S'):
        img_id = img_id.replace('S', 'Superficial-Intermediate_')
        path = '/content/drive/MyDrive/sipakmed_dataset/detection_dataset/images/im_' + img_id + '.bmp'
    elif (img_id[0] == 'P'):
        img_id = img_id.replace('P', 'Parabasal_')
        path = '/content/drive/MyDrive/sipakmed_dataset/detection_dataset/images/im_' + img_id + '.bmp'
    elif (img_id[0] == 'K'):
        img_id = img_id.replace('K', 'Koilocytotic_')
        path = '/content/drive/MyDrive/sipakmed_dataset/detection_dataset/images/im_' + img_id + '.bmp'
    elif (img_id[0] == 'M'):
        img_id = img_id.replace('M', 'Metaplastic_')
        path = '/content/drive/MyDrive/sipakmed_dataset/detection_dataset/images/im_' + img_id + '.bmp'
    elif (img_id[0] == 'D'):
        img_id = img_id.replace('D', 'Dyskeratotic_')
        path = '/content/drive/MyDrive/sipakmed_dataset/detection_dataset/images/im_' + img_id + '.bmp'

    return path

def path_from_id_pair(img_id, c_mem):
    if (img_id[0] == 'S'):
        img_id = img_id.replace('S', 'Superficial-Intermediate_')
        path = '/content/drive/MyDrive/sipakmed_dataset/detection_dataset/boundaries/im_' + img_id + '_cyt' + c_mem + '.dat'
    elif (img_id[0] == 'P'):
        img_id = img_id.replace('P', 'Parabasal_')
        path = '/content/drive/MyDrive/sipakmed_dataset/detection_dataset/boundaries/im_' + img_id + '_cyt' + c_mem + '.dat'
    elif (img_id[0] == 'K'):
        img_id = img_id.replace('K', 'Koilocytotic_')
        path = '/content/drive/MyDrive/sipakmed_dataset/detection_dataset/boundaries/im_' + img_id + '_cyt' + c_mem + '.dat'
    elif (img_id[0] == 'M'):
        img_id = img_id.replace('M', 'Metaplastic_')
        path = '/content/drive/MyDrive/sipakmed_dataset/detection_dataset/boundaries/im_' + img_id + '_cyt' + c_mem + '.dat'
    elif (img_id[0] == 'D'):
        img_id = img_id.replace('D', 'Dyskeratotic_')
        path = '/content/drive/MyDrive/sipakmed_dataset/detection_dataset/boundaries/im_' + img_id + '_cyt' + c_mem + '.dat'

    return path

def crowd_size(img_id):
    if (img_id[0] == 'S'):
        img_id = img_id.replace('S', 'Superficial-Intermediate_')
        path = '/content/drive/MyDrive/sipakmed_dataset/detection_dataset/boundaries/im_' + img_id + '*'
    elif (img_id[0] == 'P'):
        img_id = img_id.replace('P', 'Parabasal_')
        path = '/content/drive/MyDrive/sipakmed_dataset/detection_dataset/boundaries/im_' + img_id + '*'
    elif (img_id[0] == 'K'):
        img_id = img_id.replace('K', 'Koilocytotic_')
        path = '/content/drive/MyDrive/sipakmed_dataset/detection_dataset/boundaries/im_' + img_id + '*'
    elif (img_id[0] == 'M'):
        img_id = img_id.replace('M', 'Metaplastic_')
        path = '/content/drive/MyDrive/sipakmed_dataset/detection_dataset/boundaries/im_' + img_id + '*'
    elif (img_id[0] == 'D'):
        img_id = img_id.replace('D', 'Dyskeratotic_')
        path = '/content/drive/MyDrive/sipakmed_dataset/detection_dataset/boundaries/im_' + img_id + '*'

    crowd = glob.glob(path)
    return len(crowd)

def extract_boundaries(img_id, c_mem):
    path = path_from_id_pair(img_id, c_mem)
    
    temp = [i.strip().split(',') for i in open(path).readlines()]
    temp_np = np.array(temp)
    a, b = temp_np.shape

    np_arr = np.zeros([a, b], dtype=np.float64)

    for i in range(a):
        for j in range(b):
            k = temp_np[i, j].astype(np.float)
            np_arr[i, j] = k

    x_max = np.amax(np_arr[:,0])
    x_min = np.amin(np_arr[:,0])
    y_max = np.amax(np_arr[:,1])
    y_min = np.amin(np_arr[:,1])

    return x_min, x_max, y_min, y_max

def create_segmentation(img_id, crowd_member):
    if (img_id[0] == 'S'):
        img_id = img_id.replace('S', 'Superficial-Intermediate_')
        path = '/content/drive/MyDrive/sipakmed_dataset/detection_dataset/boundaries/im_' + img_id + '_cyt' + crowd_member + '.dat'
    elif (img_id[0] == 'P'):
        img_id = img_id.replace('P', 'Parabasal_')
        path = '/content/drive/MyDrive/sipakmed_dataset/detection_dataset/boundaries/im_' + img_id + '_cyt' + crowd_member + '.dat'
    elif (img_id[0] == 'K'):
        img_id = img_id.replace('K', 'Koilocytotic_')
        path = '/content/drive/MyDrive/sipakmed_dataset/detection_dataset/boundaries/im_' + img_id + '_cyt' + crowd_member + '.dat'
    elif (img_id[0] == 'M'):
        img_id = img_id.replace('M', 'Metaplastic_')
        path = '/content/drive/MyDrive/sipakmed_dataset/detection_dataset/boundaries/im_' + img_id + '_cyt' + crowd_member + '.dat'
    elif (img_id[0] == 'D'):
        img_id = img_id.replace('D', 'Dyskeratotic_')
        path = '/content/drive/MyDrive/sipakmed_dataset/detection_dataset/boundaries/im_' + img_id + '_cyt' + crowd_member + '.dat'

    temp = [i.strip().split(',') for i in open(path).readlines()]
    temp_np = np.array(temp)
    a, b = temp_np.shape

    np_arr = np.zeros([a, b], dtype=np.float64)

    for i in range(a):
        for j in range(b):
            k = temp_np[i, j].astype(np.float)
            np_arr[i, j] = k
    
    segmentation = []
    for i in range(a - 1):
        xn = np_arr[i, 0]
        yn = np_arr[i, 1]
        segmentation.append(xn)
        segmentation.append(yn)
    
    return segmentation

def extract_dim(img_id, crowd_member):
  x_min, x_max, y_min, y_max = extract_boundaries(img_id, crowd_member)
  width = x_max - x_min
  height = y_max - y_min
  
  return width, height

def create_area(img_id, crowd_member):
  width, height = extract_dim(img_id, crowd_member)

  return width * height

def create_bbox(img_id, crowd_member):
    x_min, x_max , y_min, y_max = extract_boundaries(img_id, crowd_member)
    width, height = extract_dim(img_id, crowd_member)
    bbox = [x_min, y_min, width, height]

    return bbox

#debugging
def are_b_fine(path):
    temp = [i.strip().split(',') for i in open(path).readlines()]
    temp_np = np.array(temp)
    if temp_np.ndim != 2:
      return path
    else:
      return True

#debugging
files = glob.glob('/content/drive/MyDrive/sipakmed_dataset/detection_dataset/boundaries/*')
corrupted = []
j = 0

for file in files:
  j += 1
  if (j % 100) == 0:
    print(j)

  path = os.path.join('/content/drive/MyDrive/sipakmed_dataset/detection_dataset/boundaries', file)
  if are_b_fine(path) == True:
    continue
  else:
    corrupted.append(path)

print("done")

def create_id_list(dir):
  id_list = []
  
  files = glob.glob(dir + '/*')
  for file in files:
    path = os.path.join(dir, file)
    img_id = create_img_id(path)
    c_size = crowd_size(img_id)

    for i in range(c_size):
        if i <= 8:
            c_mem = '0' + str(i + 1)
        else:
            c_mem = str(i + 1)
        id_pair = [img_id, c_mem]
        id_list.append(id_pair)

  return id_list

len_seg = seg_len_b
dir_seg = '/content/drive/MyDrive/sipakmed_dataset/detection_dataset/images'
len_seg_train = int(len_seg * 0.7)
len_seg_val = int(len_seg * 0.15)
len_seg_test = len_seg - len_seg_train - len_seg_val
print(len_seg_train, len_seg_val, len_seg_test)

id_list = create_id_list(dir_seg)
random.shuffle(id_list)
id_list_train = id_list[0:len_seg_train]
id_list_val = id_list[len_seg_train:(len_seg_train + len_seg_val)]
id_list_test = id_list[(len_seg_train + len_seg_val):(len_seg)]
print(len(id_list_train), len(id_list_val), len(id_list_test))

# adapting segmentation dataset to COCO by creating dicts to be converted to json files
def get_dicts_from_dir(dir):
    dict_list = []
    files = glob.glob(dir + '/images/*')
    j = 0

    for file in files:
      j += 1

      if (j % 100) == 0:
          print("%d / %d " % (j, len_seg) )

      path = os.path.join(dir + '/images',file)
      if (file.endswith('.bmp') == False):
        os.remove(path)
        continue

      else:
          img_id = create_img_id(path)
          c_size = crowd_size(img_id)
          _, w, h = check_res(path)

          if c_size == 1:
            is_crowd = 0
          else:
            is_crowd = 1

          annotations = []

          for i in range(c_size):
              if i <= 8:
                c_mem = '0' + str(i + 1)
              else:
                c_mem = str(i + 1)

              bbox = create_bbox(img_id, c_mem)
              segmentation = create_segmentation(img_id, c_mem)
              
              annot_dict = {
                  "bbox" : bbox,
                  "bbox_mode" : 1,
                  "category_id" : 0,
                  "segmentation" : segmentation,
                  "is_crowd" : is_crowd
              } 

              annotations.append(annot_dict)

          img_dict = {
              "filename" : path,
              "height" :  h,
              "width" : w,
              "image_id" : img_id,
              "annotations" : annotations
          }

          dict_list.append(img_dict)
      
    return dict_list

def get_dicts_from_list(list):
  dict_list = []
  j = 0

  for id in list:
    j += 1
    if (j % 100) == 0:
          print("%d / %d " % (j, len(list)) )

    img_id = id[0]
    c_mem = id[1]
    path = path_from_img_id(img_id)
    c_size = crowd_size(img_id)
    _, w, h = check_res(path)

    if c_size == 1:
        is_crowd = 0
    else:
        is_crowd = 1

    annotations = []

    bbox = create_bbox(img_id, c_mem)
    segmentation = create_segmentation(img_id, c_mem)
              
    annot_dict = {
                  "bbox" : bbox,
                  "bbox_mode" : 1,
                  "category_id" : 0,
                  "segmentation" : segmentation,
                  "is_crowd" : is_crowd
                  } 

    annotations.append(annot_dict)

    img_dict = {
                "filename" : path,
                "height" :  h,
                "width" : w,
                "image_id" : img_id,
                "annotations" : annotations
                }

    dict_list.append(img_dict)

  return dict_list

#debugging
def check_bound_f_presence(id):
    img_id = id[0]
    c_mem = id[1]

    path = path_from_id_pair(img_id, c_mem)
    files = glob.glob(path)

    if files == []:
      return False
    else:
      return True

#debugging
missing = []

for id in id_list:
  if check_bound_f_presence(id) == False:
    missing.append(id)

#creating dics and saving jsons
dict_list_train = get_dicts_from_list(id_list_train)
print("dict_list_train done")

with open('seg_train.json', 'w') as fout:
    json.dump(dict_list_train, fout)

dict_list_val = get_dicts_from_list(id_list_val)
print("dict_list_val done")

with open('seg_val.json', 'w') as fout:
    json.dump(dict_list_val, fout)

dict_list_test = get_dicts_from_list(id_list_test)
print("dict_list_test done")

with open('seg_test.json', 'w') as fout:
    json.dump(dict_list_test, fout)

with open('seg_train.json', 'w') as fout:
    json.dump(dict_list_train, fout)
  
with open('seg_val.json', 'w') as fout:
    json.dump(dict_list_val, fout)
  
with open('seg_test.json', 'w') as fout:
    json.dump(dict_list_test, fout)

print(len(dict_list_train), len(dict_list_val), len(dict_list_test))

"""## Detection/segmentation"""

import detectron2
from detectron2.utils.logger import setup_logger
setup_logger()

from detectron2 import model_zoo
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.utils.visualizer import Visualizer
from detectron2.data import MetadataCatalog, DatasetCatalog
from detectron2.structures import BoxMod
from detectron2.data.datasets import register_coco_instances



register_coco_instances("my_dataset_train", {}, "/content/seg_train.json", "/content/drive/MyDrive/sipakmed_dataset/detection_dataset/images")
register_coco_instances("my_dataset_val", {}, "/content/seg_val.json", "/content/drive/MyDrive/sipakmed_dataset/detection_dataset/images")
register_coco_instances("my_dataset_test", {}, "/content/seg_test.json", "/content/drive/MyDrive/sipakmed_dataset/detection_dataset/images")

cfg = get_cfg()
cfg.merge_from_file(model_zoo.get_config_file("COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml"))
cfg.DATASETS.TRAIN = ("my_dataset_train",)
cfg.DATASETS.TEST = ("my_dataset_val",)

cfg.DATALOADER.NUM_WORKERS = 4
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml")  # Let training initialize from model zoo
cfg.SOLVER.IMS_PER_BATCH = 4
cfg.SOLVER.BASE_LR = 0.001


cfg.SOLVER.WARMUP_ITERS = 1000
cfg.SOLVER.MAX_ITER = 1500 #adjust up if val mAP is still rising, adjust down if overfit
cfg.SOLVER.STEPS = (1000, 1500)
cfg.SOLVER.GAMMA = 0.05

cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 64
cfg.MODEL.ROI_HEADS.NUM_CLASSES = 4

cfg.TEST.EVAL_PERIOD = 500

"""## Classification"""

